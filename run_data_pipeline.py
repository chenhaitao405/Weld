#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import subprocess
import sys
import json
from typing import List, Dict
from collections import OrderedDict
from pathlib import Path
import platform
from tqdm import tqdm
import shutil
import argparse
import copy
import yaml

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DEFAULT_CONFIG_PATH = os.path.join(CURRENT_DIR, "configs", "pipeline_profiles_new.yaml")

CONFIG_PATH = DEFAULT_CONFIG_PATH
ACTIVE_PROFILE_NAME = None
BASE_PATH = ""
JSON_BASE_PATH = ""
OUTPUT_BASE_DIR = ""
REFERENCE_LABEL_MAP_PATH = "/datasets/PAR/Xray/self/1120/labeled/roi2_merge/yolo/dataset.yaml"
DATASETS: List[str] = []
DATASET_PAIRS: List[Dict[str, str]] = []
DATASET_ITEMS: List[Dict[str, str]] = []
OUTPUT_CONFIG: Dict[str, str] = {}
INTERMEDIATE_OUTPUTS: Dict[str, str] = {}
FIXED_PARAMS: Dict[str, Dict] = {}
PARAM_LOG_PATH = ""
PARAM_LOG: Dict = {}
PIPELINE_SEED = None
KEEP_INTERMEDIATE = False

def _ensure_log_dir():
    if not PARAM_LOG_PATH:
        return
    os.makedirs(os.path.dirname(PARAM_LOG_PATH), exist_ok=True)

def save_param_log():
    """æŒä¹…åŒ–æµæ°´çº¿å‚æ•°è®°å½•"""
    if not PARAM_LOG_PATH:
        return
    _ensure_log_dir()
    with open(PARAM_LOG_PATH, 'w', encoding='utf-8') as f:
        json.dump(PARAM_LOG, f, ensure_ascii=False, indent=2)

def log_command(step_name: str, command: List[str], param_key: str = None,
                extra_info: Dict = None):
    """è®°å½•è„šæœ¬è°ƒç”¨åŠå…¶è¾“å…¥å‚æ•°"""
    arguments = command[2:] if len(command) > 2 else []
    params = {}
    if param_key and param_key in FIXED_PARAMS:
        params = copy.deepcopy(FIXED_PARAMS[param_key])
        params.pop("script_path", None)

    entry = {
        "step": step_name,
        "arguments": arguments,
    }

    if params:
        entry["params"] = params

    if extra_info:
        entry["extra"] = extra_info

    PARAM_LOG["commands"].append(entry)
    save_param_log()

# å®šä¹‰æ­¥éª¤ä¿¡æ¯ï¼ˆåˆå¹¶æ­¥éª¤ï¼š23 ä¸ 456ï¼‰
STEP_INFO = {
    '1': {
        'name': 'Labelmeè½¬YOLO',
        'func': 'step1_labelme2yolo',
        'input': None,
        'output': 'yolo_dir'
    },
    '23': {
        'name': 'YOLO ROIæå– + ç«–å›¾æ—‹è½¬',
        'func': 'step23_roi_rotate',
        'input': 'yolo_dir',
        'output': 'roi_rotate'
    },
    '456': {
        'name': 'è£å‰ªå¢å¼º + seg2det + YOLOè½¬COCO',
        'func': 'step456_patch_seg2det_coco',
        'input': 'roi_rotate',
        'output': 'coco_dir'
    },
    '7': {
        'name': 'COCOæ•°æ®é›†åˆå¹¶',
        'func': 'step7_merge_coco',
        'input': 'coco_dir',
        'output': 'merged_coco_dir'
    }
}

def resolve_path(path_value: str, base_dir: str = None) -> str:
    """å°†è·¯å¾„è§£æä¸ºç»å¯¹è·¯å¾„ï¼Œå¿…è¦æ—¶ç›¸å¯¹ base_dir."""
    if path_value is None:
        return None

    expanded = os.path.expanduser(str(path_value))
    if os.path.isabs(expanded):
        return os.path.abspath(expanded)

    if base_dir:
        return os.path.abspath(os.path.join(base_dir, expanded))

    return os.path.abspath(expanded)


def _normalize_dataset_pairs(raw_pairs: List[Dict[str, str]], base_dir: str) -> List[Dict[str, str]]:
    """è§„èŒƒåŒ– dataset_pairs é…ç½®å¹¶è§£æè·¯å¾„."""
    normalized: List[Dict[str, str]] = []
    for idx, pair in enumerate(raw_pairs):
        if not isinstance(pair, dict):
            raise ValueError(f"dataset_pairs[{idx}] å¿…é¡»æ˜¯å­—å…¸")

        image_root_raw = pair.get("image_root") or pair.get("images") or pair.get("image_dir")
        label_root_raw = pair.get("label_root") or pair.get("labels") or pair.get("label_dir")
        if not image_root_raw or not label_root_raw:
            raise ValueError(f"dataset_pairs[{idx}] éœ€è¦åŒ…å« image_root ä¸ label_root")

        normalized.append({
            "name": pair.get("name"),
            "image_root": resolve_path(image_root_raw, base_dir),
            "label_root": resolve_path(label_root_raw, base_dir),
        })
    return normalized


def _has_json_files(target_dir: str) -> bool:
    if not target_dir or not os.path.isdir(target_dir):
        return False
    for name in os.listdir(target_dir):
        if name.endswith(".json"):
            return True
    return False


def _resolve_label_dir(base_dir: str) -> str:
    """ä¼˜å…ˆä½¿ç”¨ base_dir/labelï¼Œå¦åˆ™å›é€€åˆ° base_dirã€‚"""
    if not base_dir or not os.path.isdir(base_dir):
        return ""
    label_dir = os.path.join(base_dir, "label")
    if _has_json_files(label_dir):
        return label_dir
    if _has_json_files(base_dir):
        return base_dir
    if os.path.isdir(label_dir):
        return label_dir
    return base_dir


def _discover_dataset_items_from_pairs(pairs: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """ä» dataset_pairs ä¸­å‘ç°å­æ•°æ®é›†ï¼Œå¹¶ç”Ÿæˆ json_dir/image_dir å¯¹ã€‚"""
    items: List[Dict[str, str]] = []
    for pair in pairs:
        image_root = pair["image_root"]
        label_root = pair["label_root"]
        pair_tag = pair.get("name") or os.path.basename(label_root.rstrip(os.sep)) or os.path.basename(image_root.rstrip(os.sep))

        if not os.path.exists(label_root):
            print(f"âš ï¸ è·³è¿‡ {label_root}ï¼šæ ‡æ³¨ç›®å½•ä¸å­˜åœ¨")
            continue
        if not os.path.exists(image_root):
            print(f"âš ï¸ è·³è¿‡ {image_root}ï¼šå›¾åƒç›®å½•ä¸å­˜åœ¨")
            continue

        subdirs = sorted([name for name in os.listdir(label_root)
                          if os.path.isdir(os.path.join(label_root, name))])
        if not subdirs:
            print(f"âš ï¸ æœªåœ¨ {label_root} ä¸‹æ‰¾åˆ°å­ç›®å½•")
            continue

        for sub in subdirs:
            json_base = os.path.join(label_root, sub)
            json_dir = _resolve_label_dir(json_base)
            image_dir = os.path.join(image_root, sub)
            if not os.path.isdir(image_dir):
                print(f"âš ï¸ è·³è¿‡ {pair_tag}/{sub}ï¼šå›¾åƒç›®å½•ä¸å­˜åœ¨ {image_dir}")
                continue
            if not _has_json_files(json_dir):
                print(f"âš ï¸ è·³è¿‡ {pair_tag}/{sub}ï¼šæœªæ‰¾åˆ°JSONæ ‡æ³¨æ–‡ä»¶ {json_dir}")
                continue
            name = f"{pair_tag}/{sub}" if pair_tag else sub
            items.append({
                "name": name,
                "json_dir": json_dir,
                "image_dir": image_dir
            })
    return items


def _build_dataset_items_from_legacy(datasets: List[str], base_path: str, json_base_path: str) -> List[Dict[str, str]]:
    """å…¼å®¹æ—§çš„ datasets + json_base_path é…ç½®."""
    items: List[Dict[str, str]] = []
    for dataset in datasets:
        items.append({
            "name": dataset,
            "json_dir": os.path.join(json_base_path, dataset, "label"),
            "image_dir": os.path.join(base_path, dataset),
        })
    return items


def load_pipeline_profile(config_path: str, requested_profile: str = None) -> str:
    """è¯»å–é…ç½®æ–‡ä»¶å¹¶åº”ç”¨æŒ‡å®š profile."""
    config_path = resolve_path(config_path or DEFAULT_CONFIG_PATH)
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{config_path}")

    with open(config_path, "r", encoding="utf-8") as f:
        config_data = yaml.safe_load(f) or {}

    if not isinstance(config_data, dict):
        raise ValueError("é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›ä¸ºå­—å…¸ç»“æ„")

    profiles = config_data.get("profiles")
    if not isinstance(profiles, dict) or not profiles:
        raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘ profiles å®šä¹‰")

    profile_name = requested_profile or config_data.get("default_profile")
    if not profile_name:
        current_platform = platform.system()
        for name, profile_data in profiles.items():
            if profile_data.get("platform") == current_platform:
                profile_name = name
                break

    if not profile_name:
        profile_name = next(iter(profiles.keys()))

    if profile_name not in profiles:
        raise KeyError(f"é…ç½®æ–‡ä»¶ä¸­ä¸å­˜åœ¨ profile: {profile_name}")

    apply_profile(config_path, profile_name, profiles[profile_name])
    return profile_name


def apply_profile(config_path: str, profile_name: str, profile_data: Dict):
    """æ ¹æ® profile è®¾ç½®å…¨å±€è·¯å¾„å’Œå‚æ•°."""
    global CONFIG_PATH, ACTIVE_PROFILE_NAME, BASE_PATH, JSON_BASE_PATH
    global OUTPUT_BASE_DIR, DATASETS, DATASET_PAIRS, DATASET_ITEMS
    global OUTPUT_CONFIG, INTERMEDIATE_OUTPUTS, FIXED_PARAMS
    global PARAM_LOG_PATH, PARAM_LOG
    global REFERENCE_LABEL_MAP_PATH

    paths_section = profile_data.get("paths") or {}
    base_path_raw = paths_section.get("base_path")
    if not base_path_raw:
        raise ValueError(f"profile {profile_name} ç¼ºå°‘ paths.base_path")
    json_base_raw = paths_section.get("json_base_path")

    output_base_raw = paths_section.get("output_base_dir") or "pipeline_outputs"
    labelme_params = (profile_data.get("params") or {}).get("labelme2yolo", {})
    reference_label_map_raw = paths_section.get("reference_label_map_path")
    if not reference_label_map_raw and not labelme_params.get("unify_to_crack"):
        print(f"âš ï¸ è­¦å‘Šï¼šprofile {profile_name} æœªé…ç½® reference_label_map_pathï¼Œå°†è‡ªåŠ¨æ‰«ææ ‡ç­¾")

    CONFIG_PATH = config_path
    ACTIVE_PROFILE_NAME = profile_name
    BASE_PATH = resolve_path(base_path_raw)
    JSON_BASE_PATH = resolve_path(json_base_raw, BASE_PATH) if json_base_raw else ""
    OUTPUT_BASE_DIR = resolve_path(output_base_raw, BASE_PATH)
    REFERENCE_LABEL_MAP_PATH = resolve_path(reference_label_map_raw, BASE_PATH) if reference_label_map_raw else ""

    dataset_pairs_raw = profile_data.get("dataset_pairs")
    if dataset_pairs_raw:
        if not isinstance(dataset_pairs_raw, list):
            raise ValueError(f"profile {profile_name} çš„ dataset_pairs å¿…é¡»æ˜¯åˆ—è¡¨")
        DATASET_PAIRS = _normalize_dataset_pairs(dataset_pairs_raw, BASE_PATH)
        DATASET_ITEMS = _discover_dataset_items_from_pairs(DATASET_PAIRS)
        DATASETS = [item["name"] for item in DATASET_ITEMS]
    else:
        if not json_base_raw:
            raise ValueError(f"profile {profile_name} ç¼ºå°‘ paths.json_base_path")
        datasets = profile_data.get("datasets") or []
        if not isinstance(datasets, list):
            raise ValueError(f"profile {profile_name} çš„ datasets å¿…é¡»æ˜¯åˆ—è¡¨")
        DATASETS = list(datasets)
        DATASET_PAIRS = []
        DATASET_ITEMS = _build_dataset_items_from_legacy(DATASETS, BASE_PATH, JSON_BASE_PATH)

    outputs_section = profile_data.get("outputs") or {}
    if not isinstance(outputs_section, dict) or not outputs_section:
        raise ValueError(f"profile {profile_name} ç¼ºå°‘ outputs å®šä¹‰")
    resolved_outputs: Dict[str, str] = {}
    for key, value in outputs_section.items():
        if value is None:
            raise ValueError(f"profile {profile_name} ä¸­ outputs.{key} ä¸ºç©º")
        resolved_outputs[key] = resolve_path(value, OUTPUT_BASE_DIR)
    OUTPUT_CONFIG = resolved_outputs

    # åˆå§‹åŒ–ä¸­é—´è¾“å‡ºç›®å½•ï¼ˆå¯é€‰è¦†ç›–ï¼‰
    tmp_root = os.path.join(OUTPUT_BASE_DIR, "_tmp")
    INTERMEDIATE_OUTPUTS = {
        "roi_dir": resolve_path(outputs_section.get("roi_dir") or "ROI", OUTPUT_BASE_DIR)
        if "roi_dir" in outputs_section else os.path.join(tmp_root, "ROI"),
        "patch_dir": resolve_path(outputs_section.get("patch_dir") or "patch", OUTPUT_BASE_DIR)
        if "patch_dir" in outputs_section else os.path.join(tmp_root, "patch"),
        "cls_dir": resolve_path(outputs_section.get("cls_dir") or "patch_det", OUTPUT_BASE_DIR)
        if "cls_dir" in outputs_section else os.path.join(tmp_root, "patch_det"),
    }

    FIXED_PARAMS = copy.deepcopy(profile_data.get("params") or {})

    param_log_raw = profile_data.get("param_log_path")
    PARAM_LOG_PATH = resolve_path(param_log_raw, OUTPUT_BASE_DIR) if param_log_raw else os.path.join(OUTPUT_BASE_DIR, "pipeline_params.json")

    required_outputs = {info["output"] for info in STEP_INFO.values() if info.get("output")}
    missing_outputs = sorted(key for key in required_outputs if key not in OUTPUT_CONFIG)
    if missing_outputs:
        raise ValueError(f"profile {profile_name} ç¼ºå°‘ä»¥ä¸‹è¾“å‡ºç›®å½•é…ç½®ï¼š{', '.join(missing_outputs)}")

    PARAM_LOG = {
        "config_path": CONFIG_PATH,
        "config_profile": ACTIVE_PROFILE_NAME,
        "base_path": BASE_PATH,
        "json_base_path": JSON_BASE_PATH,
        "reference_label_map_path": REFERENCE_LABEL_MAP_PATH,
        "datasets": list(DATASETS),
        "dataset_pairs": list(DATASET_PAIRS),
        "dataset_items": list(DATASET_ITEMS),
        "output_base_dir": OUTPUT_BASE_DIR,
        "selected_steps": [],
        "commands": []
    }

# ===========================================================================

def load_label_map_from_yaml(yaml_path: str) -> OrderedDict:
    """ä» dataset.yaml è¯»å– label_id_mapã€‚"""
    if not yaml_path:
        raise ValueError("ç¼ºå°‘å‚è€ƒ dataset.yaml è·¯å¾„")

    yaml_file = Path(yaml_path)
    if not yaml_file.exists():
        raise FileNotFoundError(f"å‚è€ƒ dataset.yaml ä¸å­˜åœ¨: {yaml_file}")

    try:
        with yaml_file.open("r", encoding="utf-8") as f:
            yaml_data = yaml.safe_load(f)
    except yaml.YAMLError as err:
        raise RuntimeError(f"è§£æ {yaml_file} å¤±è´¥: {err}") from err

    label_map_raw = yaml_data.get("label_id_map") if yaml_data else None
    if not isinstance(label_map_raw, dict):
        raise ValueError(f"{yaml_file} ç¼ºå°‘æœ‰æ•ˆçš„ label_id_map")

    ordered_pairs = sorted(label_map_raw.items(), key=lambda item: item[1])
    return OrderedDict(ordered_pairs)

# ===========================================================================

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='æ•°æ®å¤„ç†æµæ°´çº¿æ§åˆ¶è„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•ï¼š
  python %(prog)s --steps 1234567  # è¿è¡Œå…¨éƒ¨ï¼ˆä¼šåˆå¹¶ä¸º 1/23/456/7ï¼‰
  python %(prog)s --steps 12       # åªè¿è¡Œæ­¥éª¤1å’Œæ­¥éª¤23
  python %(prog)s --steps 456      # åªè¿è¡Œæ­¥éª¤456
  python %(prog)s --steps 7        # åªè¿è¡Œåˆå¹¶åçš„COCO

æ­¥éª¤è¯´æ˜ï¼ˆåˆå¹¶ç‰ˆï¼‰ï¼š
  1   : Labelmeè½¬YOLOæ ¼å¼
  23  : YOLO ROIæå– + ç«–å›¾æ—‹è½¬
  456 : å›¾åƒè£å‰ªå¢å¼º + seg2det + YOLOâ†’COCO
  7   : COCOæ•°æ®é›†åˆå¹¶
        """
    )
    
    parser.add_argument(
        '--steps',
        type=str,
        default='1234567',
        help='è¦æ‰§è¡Œçš„æ­¥éª¤ç¼–å·ï¼Œå¦‚ "1234567" æ‰§è¡Œå…¨éƒ¨ï¼Œ"1234" æ‰§è¡Œå‰å››æ­¥ (é»˜è®¤: 1234567)'
    )
    
    parser.add_argument(
        '--force',
        action='store_true',
        help='å¼ºåˆ¶æ‰§è¡Œæ­¥éª¤ï¼Œå³ä½¿å‰ç½®ä¾èµ–çš„è¾“å‡ºç›®å½•ä¸å­˜åœ¨'
    )

    parser.add_argument(
        '--seed',
        type=int,
        default=None,
        help='éšæœºç§å­ï¼ˆç”¨äºæ•°æ®åˆ’åˆ†ã€å¹³è¡¡ç­‰å¯å¤ç°æ§åˆ¶ï¼‰'
    )

    parser.add_argument(
        '--keep-intermediate',
        action='store_true',
        help='ä¿ç•™ä¸­é—´äº§ç‰©ç›®å½•ï¼ˆé»˜è®¤æ¸…ç† ROI/patch/cls ç­‰ä¸­é—´è¾“å‡ºï¼‰'
    )

    parser.add_argument(
        '--config-path',
        type=str,
        default=DEFAULT_CONFIG_PATH,
        help=f'é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {DEFAULT_CONFIG_PATH})'
    )

    parser.add_argument(
        '--profile',
        type=str,
        default=None,
        help='é…ç½®æ–‡ä»¶ä¸­è¦ä½¿ç”¨çš„ profile åç§°ï¼ˆé»˜è®¤ä½¿ç”¨ default_profile æˆ–æ“ä½œç³»ç»ŸåŒ¹é…é¡¹ï¼‰'
    )

    return parser.parse_args()

def validate_steps(steps_str: str) -> List[str]:
    """éªŒè¯å¹¶è¿”å›è¦æ‰§è¡Œçš„æ­¥éª¤åˆ—è¡¨ï¼ˆåˆå¹¶ 23 / 456ï¼‰"""
    valid_steps = set('1234567')
    requested = []

    for char in steps_str:
        if char in valid_steps:
            if char not in requested:  # é¿å…é‡å¤
                requested.append(char)
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šå¿½ç•¥æ— æ•ˆçš„æ­¥éª¤ç¼–å· '{char}'")

    if not requested:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„æ­¥éª¤å¯æ‰§è¡Œï¼")
        sys.exit(1)

    merged_steps: List[str] = []
    if '1' in requested:
        merged_steps.append('1')
    if '2' in requested or '3' in requested:
        merged_steps.append('23')
    if '4' in requested or '5' in requested or '6' in requested:
        merged_steps.append('456')
    if '7' in requested:
        merged_steps.append('7')

    return merged_steps

def collect_all_labels(dataset_items: List[Dict[str, str]],
                       unify_to_crack: bool = False,
                       reference_label_map_path: str = None) -> OrderedDict:
    """
    æ”¶é›†æ‰€æœ‰æ•°æ®é›†çš„æ ‡ç­¾ï¼Œå»ºç«‹ç»Ÿä¸€çš„æ ‡ç­¾æ˜ å°„
    """
    # å¦‚æœå¯ç”¨äº†unify_to_crackï¼Œç›´æ¥è¿”å›crackæ˜ å°„
    if unify_to_crack:
        print("\nğŸ“Š å¯ç”¨äº† unify_to_crackï¼Œæ‰€æœ‰æ ‡ç­¾å°†ç»Ÿä¸€ä¸º 'crack'")
        label_map = OrderedDict([('crack', 0)])
        print(f"ğŸ“‹ ç»Ÿä¸€æ ‡ç­¾æ˜ å°„ï¼š{dict(label_map)}")
        return label_map

    if reference_label_map_path:
        print("\nğŸ“Š ä»å‚è€ƒ dataset.yaml è¯»å–æ ‡ç­¾æ˜ å°„...")
        label_map = load_label_map_from_yaml(reference_label_map_path)
        print(f"ğŸ“‹ å¼•ç”¨ {reference_label_map_path} ä¸­çš„ label_id_mapï¼š")
        for label, idx in label_map.items():
            print(f"  {idx}: {label}")
        return label_map

    print("\nğŸ“Š æ”¶é›†æ‰€æœ‰æ•°æ®é›†çš„æ ‡ç­¾...")
    all_labels = set()
    dataset_labels: Dict[str, set] = {}

    for item in dataset_items:
        dataset_name = item.get("name") or "unknown"
        json_dir = item.get("json_dir")
        if not json_dir or not os.path.exists(json_dir):
            print(f"  âš ï¸ è·³è¿‡ {dataset_name}ï¼šæ ‡æ³¨ç›®å½•ä¸å­˜åœ¨ {json_dir}")
            continue

        dataset_labels[dataset_name] = set()

        # æ‰«æè¯¥æ•°æ®é›†çš„æ‰€æœ‰JSONæ–‡ä»¶
        for json_file in os.listdir(json_dir):
            if not json_file.endswith('.json'):
                continue

            json_path = os.path.join(json_dir, json_file)
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                for shape in data.get('shapes', []):
                    label = shape.get('label', '').strip()
                    if label:
                        dataset_labels[dataset_name].add(label)
                        all_labels.add(label)

            except Exception as e:
                print(f"  âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {json_file}: {e}")

        if dataset_labels[dataset_name]:
            print(f"  âœ“ {dataset_name}: å‘ç° {len(dataset_labels[dataset_name])} ä¸ªæ ‡ç­¾")

    # åˆ›å»ºç»Ÿä¸€çš„æ ‡ç­¾æ˜ å°„
    sorted_labels = sorted(all_labels)
    label_map = OrderedDict([(label, idx) for idx, label in enumerate(sorted_labels)])

    print(f"\nğŸ“‹ ç»Ÿä¸€æ ‡ç­¾æ˜ å°„ï¼ˆå…± {len(label_map)} ä¸ªæ ‡ç­¾ï¼‰ï¼š")
    for label, idx in label_map.items():
        # æ‰¾å‡ºå“ªäº›æ•°æ®é›†åŒ…å«è¿™ä¸ªæ ‡ç­¾
        datasets_with_label = [d for d, labels in dataset_labels.items() if label in labels]
        print(f"  {idx}: {label} (å‡ºç°åœ¨: {', '.join(datasets_with_label)})")

    return label_map

def create_dataset_yaml(output_dir: str, label_map: OrderedDict):
    """åˆ›å»ºç»Ÿä¸€çš„dataset.yamlæ–‡ä»¶"""
    yaml_path = os.path.join(output_dir, "dataset.yaml")

    content = f"""# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# ç»Ÿä¸€æ•°æ®é›†é…ç½®æ–‡ä»¶

# æ•°æ®é›†è·¯å¾„
path: {output_dir}  # dataset root dir
train: images/train  # train images (relative to 'path')
val: images/val  # val images (relative to 'path')

# ç±»åˆ«
nc: {len(label_map)}  # number of classes
names: {list(label_map.keys())}  # class names

# æ ‡ç­¾IDæ˜ å°„
label_id_map: {dict(label_map)}
"""

    with open(yaml_path, 'w', encoding='utf-8') as f:
        f.write(content)

    print(f"\nâœ… åˆ›å»ºç»Ÿä¸€çš„ dataset.yaml: {yaml_path}")

def process_labelme2yolo_unified(dataset_items: List[Dict[str, str]], output_dir: str, seed: int = None):
    """
    ç›´æ¥å¤„ç†æ‰€æœ‰æ•°æ®é›†åˆ°ä¸»ç›®å½•ï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ ‡ç­¾æ˜ å°„
    """

    # è·å– unify_to_crack è®¾ç½®
    unify_to_crack = FIXED_PARAMS["labelme2yolo"].get("unify_to_crack", False)
    if unify_to_crack:
        print("\nâš ï¸ æ³¨æ„ï¼šå·²å¯ç”¨ unify_to_crackï¼Œæ‰€æœ‰æ ‡ç­¾å°†è¢«ç»Ÿä¸€ä¸º 'crack'")

    # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰æ ‡ç­¾ï¼Œå»ºç«‹ç»Ÿä¸€æ˜ å°„
    label_map = collect_all_labels(
        dataset_items,
        unify_to_crack,
        REFERENCE_LABEL_MAP_PATH
    )

    if not label_map:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•æ ‡ç­¾ï¼")
        sys.exit(1)

    # ç¬¬äºŒæ­¥ï¼šç›´æ¥å¤„ç†æ‰€æœ‰æ•°æ®é›†åˆ°ä¸»ç›®å½•
    script_path = get_abs_path(FIXED_PARAMS["labelme2yolo"]["script_path"])

    for item in dataset_items:
        dataset_name = item.get("name") or "unknown"
        image_dir = item.get("image_dir")
        json_dir = item.get("json_dir")

        if not image_dir or not json_dir or not os.path.exists(image_dir) or not os.path.exists(json_dir):
            print(f"âš ï¸ è·³è¿‡ {dataset_name}ï¼šè·¯å¾„ä¸å­˜åœ¨")
            continue

        print(f"\nå¤„ç†æ•°æ®é›†: {dataset_name}")

        command = [
            sys.executable, script_path,
            "--json_dir", json_dir,
            "--image_dir", image_dir,
            "--output_dir", output_dir,
            "--label_map", json.dumps(dict(label_map))  # ä¼ é€’ç»Ÿä¸€çš„æ ‡ç­¾æ˜ å°„
        ]

        if FIXED_PARAMS["labelme2yolo"]["seg"]:
            command.append("--seg")

        if seed is not None:
            command.extend(["--seed", str(seed)])

        # æ‰§è¡Œè½¬æ¢
        run_command(
            command,
            f"Labelmeè½¬YOLO - {dataset_name}",
            param_key="labelme2yolo",
            extra_info={"dataset": dataset_name, "json_dir": json_dir, "image_dir": image_dir}
        )

def run_command(command: List[str], step_name: str, param_key: str = None,
                extra_info: Dict = None):
    """æ‰§è¡Œå‘½ä»¤"""
    log_command(step_name, command, param_key, extra_info)
    print(f"\n{'=' * 80}")
    print(f"ğŸ“Œ æ­£åœ¨æ‰§è¡Œã€{step_name}ã€‘")
    print(f"å‘½ä»¤ï¼š{' '.join(command)}")
    print(f"{'=' * 80}")

    try:
        subprocess.run(
            command,
            check=True,
            stdout=None,
            stderr=None,
            text=True,
            env=os.environ
        )
        print(f"\nâœ… ã€{step_name}ã€‘æ‰§è¡ŒæˆåŠŸï¼")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ ã€{step_name}ã€‘æ‰§è¡Œå¤±è´¥ï¼é”™è¯¯ç ï¼š{e.returncode}")
        sys.exit(1)


def cleanup_intermediate_outputs():
    """æ¸…ç†ä¸­é—´äº§ç‰©ç›®å½•ï¼ˆä»…é™ OUTPUT_BASE_DIR ä¸‹çš„ _tmp æˆ–æ˜¾å¼ä¸­é—´ç›®å½•ï¼‰"""
    if KEEP_INTERMEDIATE:
        return

    output_root = os.path.abspath(OUTPUT_BASE_DIR)
    for key, path in INTERMEDIATE_OUTPUTS.items():
        if not path:
            continue
        abs_path = os.path.abspath(path)
        if not abs_path.startswith(output_root + os.sep):
            print(f"âš ï¸ è·³è¿‡æ¸…ç†ä¸­é—´ç›®å½•ï¼ˆä¸åœ¨è¾“å‡ºæ ¹ç›®å½•å†…ï¼‰: {abs_path}")
            continue
        if os.path.exists(abs_path):
            print(f"ğŸ§¹ æ¸…ç†ä¸­é—´ç›®å½•: {abs_path}")
            shutil.rmtree(abs_path, ignore_errors=True)

def get_abs_path(relative_path: str) -> str:
    """è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„"""
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    return os.path.abspath(os.path.join(current_script_dir, relative_path))

def process_roi_extractor(input_dir: str, output_dir: str):
    """æ‰§è¡Œ ROI æå–"""
    script_path = get_abs_path(FIXED_PARAMS["yolo_roi_extractor"]["script_path"])
    command = [
        sys.executable, script_path,
        "--input_dir", input_dir,
        "--output_dir", output_dir,
        "--model_path", FIXED_PARAMS["yolo_roi_extractor"]["model_path"],
        "--roi_conf", str(FIXED_PARAMS["yolo_roi_extractor"]["roi_conf"]),
        "--roi_iou", str(FIXED_PARAMS["yolo_roi_extractor"]["roi_iou"]),
        "--padding", str(FIXED_PARAMS["yolo_roi_extractor"]["padding"]),
        "--mode", FIXED_PARAMS["yolo_roi_extractor"]["mode"]
    ]

    run_command(command, "YOLO ROIæå–", param_key="yolo_roi_extractor")

def process_rotate_yolo(input_dir: str, output_dir: str):
    """æ‰§è¡ŒYOLOç«–å›¾æ—‹è½¬æ ‡å‡†åŒ–"""
    script_path = get_abs_path(FIXED_PARAMS["rotate_yolo"]["script_path"])
    command = [
        sys.executable, script_path,
        "--input", input_dir,
        "--output", output_dir
    ]

    run_command(command, "YOLOç«–å›¾æ—‹è½¬", param_key="rotate_yolo")

def process_patch_enhance(input_dir: str, output_dir: str, seed: int = None):
    """æ‰§è¡Œå›¾åƒè£å‰ªå¢å¼º"""
    script_path = get_abs_path(FIXED_PARAMS["patchandenhance"]["script_path"])
    patch_cfg = FIXED_PARAMS["patchandenhance"]
    slice_mode = patch_cfg.get("slice_mode")
    if slice_mode is None:
        slice_mode = 1 if patch_cfg.get("no_slice") else 2

    command = [
        sys.executable, script_path,
        "--input_dir", input_dir,
        "--output_dir", output_dir,
        "--enhance_mode", patch_cfg["enhance_mode"],
        "--label_mode", patch_cfg["label_mode"]
    ]

    if slice_mode == 2:
        command.extend([
            "--overlap", str(patch_cfg["overlap"]),
            "--window_size",
            str(patch_cfg["window_size"][0]),
            str(patch_cfg["window_size"][1])
        ])

    command.extend(["--slice_mode", str(slice_mode)])
    if seed is not None:
        command.extend(["--seed", str(seed)])

    run_command(command, "å›¾åƒè£å‰ªä¸å¢å¼º", param_key="patchandenhance")

def seg2det(input_dir: str, output_dir: str, seed: int = None):
    """æ‰§è¡Œè®­ç»ƒä»»åŠ¡è½¬æ¢"""
    seg_cfg = FIXED_PARAMS["seg2det"]
    script_path = get_abs_path(seg_cfg["script_path"])
    command = [
        sys.executable, script_path,
        "--input_dir", input_dir,
        "--output_dir", output_dir,
        "--mode", str(seg_cfg["mode"]),
    ]
    if seg_cfg.get("balance_data"):
        command.append("--balance_data")
        balance_ratio = seg_cfg.get("balance_ratio")
        if balance_ratio is not None:
            command.extend(["--balance_ratio", str(balance_ratio)])
    if seed is not None:
        command.extend(["--seed", str(seed)])

    run_command(command, "è®­ç»ƒä»»åŠ¡è½¬æ¢", param_key="seg2det")


def process_yolo2coco(input_dir: str, output_dir: str, seed: int = None):
    """æ‰§è¡Œ YOLOâ†’COCO è½¬æ¢"""
    yolo2coco_cfg = FIXED_PARAMS.get("yolo2coco")
    if not yolo2coco_cfg:
        raise KeyError("é…ç½®ç¼ºå°‘ params.yolo2coco")

    script_path = get_abs_path(yolo2coco_cfg["script_path"])
    command = [
        sys.executable, script_path,
        "--input_dir", input_dir,
        "--output_dir", output_dir
    ]

    task = yolo2coco_cfg.get("task")
    if task:
        command.extend(["--task", str(task)])
    if yolo2coco_cfg.get("test_split_ratio") is not None:
        command.extend(["--test_split_ratio", str(yolo2coco_cfg["test_split_ratio"])])
    if yolo2coco_cfg.get("split_seed") is not None:
        command.extend(["--split_seed", str(yolo2coco_cfg["split_seed"])])
    elif seed is not None:
        command.extend(["--split_seed", str(seed)])

    run_command(command, "YOLOè½¬COCO", param_key="yolo2coco")


def process_merge_coco(dataset_a_dir: str, output_dir: str):
    """æ‰§è¡Œ COCO æ•°æ®é›†åˆå¹¶"""
    merge_cfg = FIXED_PARAMS.get("merge_coco")
    if not merge_cfg:
        raise KeyError("é…ç½®ç¼ºå°‘ params.merge_coco")

    dataset_b_raw = merge_cfg.get("dataset_b")
    if not dataset_b_raw:
        raise ValueError("merge_coco.dataset_b æœªé…ç½®ï¼Œè¯·åœ¨ YAML ä¸­æŒ‡å®š")

    dataset_b_path = resolve_path(dataset_b_raw, BASE_PATH)
    script_path = get_abs_path(merge_cfg["script_path"])
    command = [
        sys.executable, script_path,
        "--dataset-a", dataset_a_dir,
        "--dataset-b", dataset_b_path,
        "--output-dir", output_dir
    ]

    splits = merge_cfg.get("splits")
    if splits:
        command.extend(["--splits"] + [str(split) for split in splits])

    if merge_cfg.get("prefix_a"):
        command.extend(["--prefix-a", str(merge_cfg["prefix_a"])])
    if merge_cfg.get("prefix_b"):
        command.extend(["--prefix-b", str(merge_cfg["prefix_b"])])
    if merge_cfg.get("copy_images"):
        command.append("--copy-images")

    merge_ratio_config = merge_cfg.get("merge_ratio")
    logged_merge_ratio = None
    if isinstance(merge_ratio_config, (list, tuple)):
        ratio_values = [str(value) for value in merge_ratio_config if value is not None]
        if ratio_values:
            command.extend(["--merge-ratio"] + ratio_values)
            logged_merge_ratio = list(merge_ratio_config)
    elif merge_ratio_config is not None:
        command.extend(["--merge-ratio", str(merge_ratio_config)])
        logged_merge_ratio = merge_ratio_config

    run_command(
        command,
        "åˆå¹¶COCOæ•°æ®é›†",
        param_key="merge_coco",
        extra_info={
            "dataset_b": str(dataset_b_path),
            "merge_ratio": logged_merge_ratio if logged_merge_ratio is not None else "default"
        }
    )

# =================== æ­¥éª¤æ‰§è¡Œå‡½æ•° ===================

def step1_labelme2yolo():
    """æ­¥éª¤1: Labelmeè½¬YOLOæ ¼å¼"""
    print("\n" + "=" * 100)
    print("ğŸ“ æ­¥éª¤1: æ‰¹é‡å¤„ç† Labelme æ•°æ®ï¼ˆä½¿ç”¨ç»Ÿä¸€æ ‡ç­¾æ˜ å°„ï¼‰")
    print("=" * 100)
    
    process_labelme2yolo_unified(
        DATASET_ITEMS,
        OUTPUT_CONFIG["yolo_dir"],
        seed=PIPELINE_SEED,
    )

def step23_roi_rotate():
    """æ­¥éª¤23: YOLO ROIæå– + ç«–å›¾æ—‹è½¬"""
    print("\n" + "=" * 100)
    print("ğŸ“ æ­¥éª¤23: æ‰§è¡Œ YOLO ROI æå– + ç«–å›¾æ—‹è½¬")
    print("=" * 100)

    if not os.path.exists(OUTPUT_CONFIG["yolo_dir"]):
        print(f"âš ï¸ è­¦å‘Šï¼šYOLO æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨ {OUTPUT_CONFIG['yolo_dir']}")
        print("  æç¤ºï¼šå¯èƒ½éœ€è¦å…ˆæ‰§è¡Œæ­¥éª¤1")

    roi_dir = INTERMEDIATE_OUTPUTS["roi_dir"]
    process_roi_extractor(OUTPUT_CONFIG["yolo_dir"], roi_dir)
    process_rotate_yolo(roi_dir, OUTPUT_CONFIG["roi_rotate"])


def step456_patch_seg2det_coco():
    """æ­¥éª¤456: å›¾åƒè£å‰ªå¢å¼º + seg2det + YOLOâ†’COCO"""
    print("\n" + "=" * 100)
    print("ğŸ“ æ­¥éª¤456: å›¾åƒè£å‰ªå¢å¼º + seg2det + YOLOâ†’COCO")
    print("=" * 100)

    if not os.path.exists(OUTPUT_CONFIG["roi_rotate"]):
        print(f"âš ï¸ è­¦å‘Šï¼šROI æ—‹è½¬ç›®å½•ä¸å­˜åœ¨ {OUTPUT_CONFIG['roi_rotate']}")
        print("  æç¤ºï¼šå¯èƒ½éœ€è¦å…ˆæ‰§è¡Œæ­¥éª¤23")

    patch_dir = INTERMEDIATE_OUTPUTS["patch_dir"]
    cls_dir = INTERMEDIATE_OUTPUTS["cls_dir"]

    process_patch_enhance(OUTPUT_CONFIG["roi_rotate"], patch_dir, seed=PIPELINE_SEED)
    seg2det(patch_dir, cls_dir, seed=PIPELINE_SEED)
    process_yolo2coco(cls_dir, OUTPUT_CONFIG["coco_dir"], seed=PIPELINE_SEED)


def step7_merge_coco():
    """æ­¥éª¤7: åˆå¹¶ COCO æ•°æ®é›†"""
    print("\n" + "=" * 100)
    print("ğŸ“ æ­¥éª¤7: åˆå¹¶ COCO æ•°æ®é›†")
    print("=" * 100)

    if not os.path.exists(OUTPUT_CONFIG["coco_dir"]):
        print(f"âš ï¸ è­¦å‘Šï¼šCOCO è½¬æ¢è¾“å‡ºä¸å­˜åœ¨ {OUTPUT_CONFIG['coco_dir']}")
        print("  æç¤ºï¼šå¯èƒ½éœ€è¦å…ˆæ‰§è¡Œæ­¥éª¤6")

    process_merge_coco(OUTPUT_CONFIG["coco_dir"], OUTPUT_CONFIG["merged_coco_dir"])

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    try:
        active_profile = load_pipeline_profile(args.config_path, args.profile)
    except Exception as exc:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥ï¼š{exc}")
        sys.exit(1)
    
    global PIPELINE_SEED, KEEP_INTERMEDIATE
    PIPELINE_SEED = args.seed
    KEEP_INTERMEDIATE = args.keep_intermediate

    print("ğŸš€ æ•°æ®å¤„ç†æµæ°´çº¿å¯åŠ¨ï¼ˆå¯æ§ç‰ˆæœ¬ï¼‰ï¼")
    print(f"é…ç½®æ–‡ä»¶ï¼š{CONFIG_PATH}")
    print(f"ä½¿ç”¨çš„profileï¼š{active_profile}")
    print(f"åŸºç¡€è·¯å¾„ï¼š{BASE_PATH}")
    if DATASET_PAIRS:
        print(f"æ•°æ®é›†å¯¹æ•°é‡ï¼š{len(DATASET_PAIRS)}")
        for pair in DATASET_PAIRS:
            print(f"  - images: {pair['image_root']} | labels: {pair['label_root']}")
        print(f"å¾…å¤„ç†å­æ•°æ®é›†æ•°é‡ï¼š{len(DATASET_ITEMS)}")
    else:
        print(f"å¾…å¤„ç†æ•°æ®é›†ï¼š{DATASETS}")
    
    # éªŒè¯æ­¥éª¤
    steps = validate_steps(args.steps)
    PARAM_LOG["selected_steps"] = list(steps)
    PARAM_LOG["seed"] = PIPELINE_SEED
    PARAM_LOG["keep_intermediate"] = KEEP_INTERMEDIATE
    save_param_log()
    
    print(f"\nğŸ“Œ å°†è¦æ‰§è¡Œçš„æ­¥éª¤ï¼š{' '.join(steps)}")
    for step in steps:
        print(f"  {step}: {STEP_INFO[step]['name']}")
    
    
    print("\n" + "=" * 100)
    print("å¼€å§‹æ‰§è¡Œé€‰å®šçš„æ­¥éª¤")
    print("=" * 100)
    
    # æ‰§è¡Œé€‰å®šçš„æ­¥éª¤
    for step in steps:
        step_func_name = STEP_INFO[step]['func']
        step_func = globals()[step_func_name]
        
        try:
            step_func()
        except Exception as e:
            print(f"\nâŒ æ­¥éª¤{step}æ‰§è¡Œå¤±è´¥ï¼š{e}")
            if not args.force:
                print("ç»ˆæ­¢æ‰§è¡Œï¼ˆä½¿ç”¨ --force å¯ä»¥ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤ï¼‰")
                sys.exit(1)
            else:
                print("ä½¿ç”¨äº† --force å‚æ•°ï¼Œç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤")

    # cleanup_intermediate_outputs()

    # å®Œæˆä¿¡æ¯
    print("\n" + "ğŸ‰" * 50)
    print("ğŸ‰ æ‰€é€‰æ­¥éª¤æ‰§è¡Œå®Œæˆï¼")
    print(f"ğŸ“ æ‰§è¡Œçš„æ­¥éª¤ï¼š{' '.join(steps)}")
    
    # æ˜¾ç¤ºå„æ­¥éª¤çš„è¾“å‡ºç›®å½•
    for step in steps:
        output_key = STEP_INFO[step]['output']
        if output_key:
            output_dir = OUTPUT_CONFIG[output_key]
            print(f"  æ­¥éª¤{step}è¾“å‡ºï¼š{output_dir}")
    
    print("ğŸ‰" * 50)

if __name__ == "__main__":
    main()
