
# !/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import subprocess
import sys
import json
from typing import List, Dict, Set
from collections import OrderedDict
from pathlib import Path
import platform
from tqdm import tqdm
import shutil

# ========================= é…ç½®åŒºåŸŸ =========================
# æ ¹æ®æ“ä½œç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©è·¯å¾„
if platform.system() == "Windows":
    BASE_PATH = r"C:\Users\CHT\Desktop\datasets1117\labeled"
    JSON_BASE_PATH = r"C:\Users\CHT\Desktop\datasets1117\adjust"
elif platform.system() == "Linux":
    BASE_PATH = "/home/lenovo/code/CHT/datasets/Xray/self/1120/labeled"
    JSON_BASE_PATH = "/home/lenovo/code/CHT/datasets/Xray/self/1120/labeled"  # ä¿®å¤å¼•å·ç¼ºå¤±é—®é¢˜
else:
    # å…¶ä»–ç³»ç»Ÿï¼ˆå¦‚macOSï¼‰å¯æ ¹æ®éœ€è¦æ·»åŠ é…ç½®ï¼Œè¿™é‡ŒæŠ›å‡ºå¼‚å¸¸æé†’
    raise EnvironmentError(
        f"ä¸æ”¯æŒçš„æ“ä½œç³»ç»Ÿï¼š{platform.system()}\n"
        "è¯·åœ¨é…ç½®åŒºåŸŸæ·»åŠ å¯¹åº”ç³»ç»Ÿçš„è·¯å¾„é…ç½®"
    )

DATASETS = [
    "D1",
    "D2",
    "D3",
    "D4",
    "img20250608",
    "img20250609"
]
OUTPUT_BASE_DIR = "unifyCrack"
OUTPUT_CONFIG = {
    "yolo_dir": os.path.join(BASE_PATH, OUTPUT_BASE_DIR,"yolo"),
    "roi_dir": os.path.join(BASE_PATH, OUTPUT_BASE_DIR,"convert"),
    "patch_dir": os.path.join(BASE_PATH,OUTPUT_BASE_DIR, "patch")
}
FIXED_PARAMS = {
    "labelme2yolo": {
        "seg": True,
        "unify_to_crack": True,  # å¦‚æœä¸ºTrueï¼Œæ‰€æœ‰æ ‡ç­¾éƒ½ä¼šè¢«ç»Ÿä¸€ä¸ºcrack
        "script_path": "convert/labelme2yolo.py"
    },
    "yolo_roi_extractor": {
        "model_path": "/home/lenovo/code/CHT/detect/ultralytics-main/runs/detect/11m_pretrain/weights/best.pt",
        "roi_conf": 0.25,
        "roi_iou": 0.45,
        "padding": 0.1,
        "mode": "seg",
        "script_path": "convert/pj/yolo_roi_extractor.py"
    },
    "patchandenhance": {
        "overlap": 0.7,
        "enhance_mode": "windowing",
        "no_slice":True,
        "window_size": [640, 640],
        "label_mode": "seg",
        "script_path": "convert/pj/patchandenhance.py"
    }
}

# ===========================================================================

def collect_all_labels(datasets: List[str], json_base_path: str,
                       unify_to_crack: bool = False) -> OrderedDict:
    """
    æ”¶é›†æ‰€æœ‰æ•°æ®é›†çš„æ ‡ç­¾ï¼Œå»ºç«‹ç»Ÿä¸€çš„æ ‡ç­¾æ˜ å°„
    """
    # å¦‚æœå¯ç”¨äº†unify_to_crackï¼Œç›´æ¥è¿”å›crackæ˜ å°„
    if unify_to_crack:
        print("\nğŸ“Š å¯ç”¨äº† unify_to_crackï¼Œæ‰€æœ‰æ ‡ç­¾å°†ç»Ÿä¸€ä¸º 'crack'")
        label_map = OrderedDict([('crack', 0)])
        print(f"ğŸ“‹ ç»Ÿä¸€æ ‡ç­¾æ˜ å°„ï¼š{dict(label_map)}")
        return label_map

    print("\nğŸ“Š æ”¶é›†æ‰€æœ‰æ•°æ®é›†çš„æ ‡ç­¾...")
    all_labels = set()
    dataset_labels = {}

    for dataset in datasets:
        json_dir = os.path.join(json_base_path, dataset, "label")
        if not os.path.exists(json_dir):
            print(f"  âš ï¸ è·³è¿‡ {dataset}ï¼šæ ‡æ³¨ç›®å½•ä¸å­˜åœ¨ {json_dir}")
            continue

        dataset_labels[dataset] = set()

        # æ‰«æè¯¥æ•°æ®é›†çš„æ‰€æœ‰JSONæ–‡ä»¶
        for json_file in os.listdir(json_dir):
            if not json_file.endswith('.json'):
                continue

            json_path = os.path.join(json_dir, json_file)
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                for shape in data.get('shapes', []):
                    label = shape.get('label', '').strip()
                    if label:
                        dataset_labels[dataset].add(label)
                        all_labels.add(label)

            except Exception as e:
                print(f"  âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {json_file}: {e}")

        if dataset_labels[dataset]:
            print(f"  âœ“ {dataset}: å‘ç° {len(dataset_labels[dataset])} ä¸ªæ ‡ç­¾")

    # åˆ›å»ºç»Ÿä¸€çš„æ ‡ç­¾æ˜ å°„
    sorted_labels = sorted(all_labels)
    label_map = OrderedDict([(label, idx) for idx, label in enumerate(sorted_labels)])

    print(f"\nğŸ“‹ ç»Ÿä¸€æ ‡ç­¾æ˜ å°„ï¼ˆå…± {len(label_map)} ä¸ªæ ‡ç­¾ï¼‰ï¼š")
    for label, idx in label_map.items():
        # æ‰¾å‡ºå“ªäº›æ•°æ®é›†åŒ…å«è¿™ä¸ªæ ‡ç­¾
        datasets_with_label = [d for d, labels in dataset_labels.items() if label in labels]
        print(f"  {idx}: {label} (å‡ºç°åœ¨: {', '.join(datasets_with_label)})")

    return label_map


def create_dataset_yaml(output_dir: str, label_map: OrderedDict):
    """åˆ›å»ºç»Ÿä¸€çš„dataset.yamlæ–‡ä»¶"""
    yaml_path = os.path.join(output_dir, "dataset.yaml")

    content = f"""# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# ç»Ÿä¸€æ•°æ®é›†é…ç½®æ–‡ä»¶

# æ•°æ®é›†è·¯å¾„
path: {output_dir}  # dataset root dir
train: images/train  # train images (relative to 'path')
val: images/val  # val images (relative to 'path')

# ç±»åˆ«
nc: {len(label_map)}  # number of classes
names: {list(label_map.keys())}  # class names

# æ ‡ç­¾IDæ˜ å°„
label_id_map: {dict(label_map)}
"""

    with open(yaml_path, 'w', encoding='utf-8') as f:
        f.write(content)

    print(f"\nâœ… åˆ›å»ºç»Ÿä¸€çš„ dataset.yaml: {yaml_path}")


def process_single_json(json_path: str, image_dir: str, label_map: OrderedDict,
                        unify_to_crack: bool, to_seg: bool) -> tuple:
    """
    å¤„ç†å•ä¸ªJSONæ–‡ä»¶ï¼Œè¿”å›YOLOæ ¼å¼çš„æ ‡æ³¨

    Returns:
        (yolo_objects, image_path, img_width, img_height)
    """
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
    except Exception as e:
        print(f"  é”™è¯¯ï¼šæ— æ³•è¯»å– {json_path}: {e}")
        return None, None, 0, 0

    img_h = json_data.get('imageHeight', 0)
    img_w = json_data.get('imageWidth', 0)

    if img_h <= 0 or img_w <= 0:
        return None, None, 0, 0

    # æŸ¥æ‰¾å¯¹åº”çš„å›¾åƒæ–‡ä»¶
    json_name = Path(json_path).stem
    image_path = None
    for ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:
        potential_path = Path(image_dir) / f"{json_name}{ext}"
        if potential_path.exists():
            image_path = str(potential_path)
            break

    if not image_path:
        return None, None, img_w, img_h

    # æå–æ ‡æ³¨
    yolo_objects = []
    for shape in json_data.get('shapes', []):
        label = shape.get('label', '').strip()

        if not label or 'points' not in shape or len(shape['points']) < 2:
            continue

        # ç»Ÿä¸€æ ‡ç­¾ä¸ºcrackï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if unify_to_crack:
            label = 'crack'

        # è·å–æ ‡ç­¾ID
        if label not in label_map:
            continue

        label_id = label_map[label]

        # å¤„ç†ä¸åŒå½¢çŠ¶ç±»å‹
        points = shape['points']

        if to_seg:
            # åˆ†å‰²æ¨¡å¼ï¼šä¿å­˜å¤šè¾¹å½¢ç‚¹
            yolo_obj = [label_id]
            for point in points:
                x_norm = round(float(point[0]) / img_w, 6)
                y_norm = round(float(point[1]) / img_h, 6)
                yolo_obj.extend([x_norm, y_norm])
        else:
            # æ£€æµ‹æ¨¡å¼ï¼šè½¬æ¢ä¸ºè¾¹ç•Œæ¡†
            x_coords = [p[0] for p in points]
            y_coords = [p[1] for p in points]

            x_min, x_max = min(x_coords), max(x_coords)
            y_min, y_max = min(y_coords), max(y_coords)

            obj_w = x_max - x_min
            obj_h = y_max - y_min

            xc = (x_min + x_max) / 2.0
            yc = (y_min + y_max) / 2.0

            yolo_obj = [
                label_id,
                round(xc / img_w, 6),
                round(yc / img_h, 6),
                round(obj_w / img_w, 6),
                round(obj_h / img_h, 6)
            ]

        yolo_objects.append(yolo_obj)

    return yolo_objects, image_path, img_w, img_h


def process_all_datasets_directly(datasets: List[str], base_path: str,
                                  json_base_path: str, output_dir: str,
                                  label_map: OrderedDict, params: dict):
    """
    ç›´æ¥å¤„ç†æ‰€æœ‰æ•°æ®é›†åˆ°ä¸»ç›®å½•ï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ ‡ç­¾æ˜ å°„
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
    os.makedirs(os.path.join(output_dir, "images/train"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, "images/val"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, "labels/train"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, "labels/val"), exist_ok=True)

    # å…ˆåˆ›å»ºç»Ÿä¸€çš„dataset.yaml
    create_dataset_yaml(output_dir, label_map)

    # æ”¶é›†æ‰€æœ‰è¦å¤„ç†çš„æ–‡ä»¶
    all_files = []

    for dataset in datasets:
        image_dir = os.path.join(base_path, dataset)
        json_dir = os.path.join(json_base_path, dataset, "label")

        if not os.path.exists(image_dir) or not os.path.exists(json_dir):
            print(f"âš ï¸ è·³è¿‡ {dataset}ï¼šè·¯å¾„ä¸å­˜åœ¨")
            continue

        # è·å–è¯¥æ•°æ®é›†çš„æ‰€æœ‰JSONæ–‡ä»¶
        json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]

        for json_file in json_files:
            all_files.append({
                'dataset': dataset,
                'json_path': os.path.join(json_dir, json_file),
                'image_dir': image_dir,
                'json_name': Path(json_file).stem
            })

    print(f"\nğŸ“ å¤„ç† {len(all_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶...")

    # éšæœºåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    import random
    random.shuffle(all_files)

    val_size = params.get('val_size', 0.1)
    val_count = int(len(all_files) * val_size)

    val_files = all_files[:val_count]
    train_files = all_files[val_count:]

    print(f"  è®­ç»ƒé›†: {len(train_files)} ä¸ªæ–‡ä»¶")
    print(f"  éªŒè¯é›†: {len(val_files)} ä¸ªæ–‡ä»¶")

    # å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    for split_name, file_list in [('train', train_files), ('val', val_files)]:
        print(f"\nå¤„ç† {split_name} é›†...")

        success_count = 0
        no_label_count = 0
        fail_count = 0

        for file_info in tqdm(file_list, desc=f"å¤„ç†{split_name}"):
            dataset = file_info['dataset']
            json_path = file_info['json_path']
            image_dir = file_info['image_dir']
            json_name = file_info['json_name']

            # å¤„ç†JSONè·å–æ ‡æ³¨
            yolo_objects, image_path, img_w, img_h = process_single_json(
                json_path, image_dir, label_map,
                params.get('unify_to_crack', False),
                params.get('seg', False)
            )

            if not image_path:
                fail_count += 1
                continue

            # å¤åˆ¶å›¾åƒï¼ˆæ·»åŠ æ•°æ®é›†å‰ç¼€ï¼‰
            src_image = Path(image_path)
            dst_image_name = f"{dataset}_{src_image.name}"
            dst_image_path = Path(output_dir) / "images" / split_name / dst_image_name
            shutil.copy2(src_image, dst_image_path)

            # ä¿å­˜æ ‡æ³¨æ–‡ä»¶
            if yolo_objects:
                label_name = f"{dataset}_{json_name}.txt"
                label_path = Path(output_dir) / "labels" / split_name / label_name

                with open(label_path, 'w') as f:
                    for obj in yolo_objects:
                        line = ' '.join(map(str, obj))
                        f.write(line + '\n')

                success_count += 1
            else:
                no_label_count += 1

        print(f"  {split_name}é›†ç»Ÿè®¡: æˆåŠŸ {success_count}, æ— æ ‡æ³¨ {no_label_count}, å¤±è´¥ {fail_count}")

    print(f"\nâœ… æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæˆï¼")


def run_command(command: List[str], step_name: str):
    """æ‰§è¡Œå‘½ä»¤"""
    print(f"\n{'=' * 80}")
    print(f"ğŸ“Œ æ­£åœ¨æ‰§è¡Œã€{step_name}ã€‘")
    print(f"å‘½ä»¤ï¼š{' '.join(command)}")
    print(f"{'=' * 80}")

    try:
        subprocess.run(
            command,
            check=True,
            stdout=None,
            stderr=None,
            text=True,
            env=os.environ
        )
        print(f"\nâœ… ã€{step_name}ã€‘æ‰§è¡ŒæˆåŠŸï¼")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ ã€{step_name}ã€‘æ‰§è¡Œå¤±è´¥ï¼é”™è¯¯ç ï¼š{e.returncode}")
        sys.exit(1)


def get_abs_path(relative_path: str) -> str:
    """è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„"""
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    return os.path.abspath(os.path.join(current_script_dir, relative_path))


def process_roi_extractor(input_dir: str, output_dir: str):
    """æ‰§è¡Œ ROI æå–"""
    script_path = get_abs_path(FIXED_PARAMS["yolo_roi_extractor"]["script_path"])
    command = [
        sys.executable, script_path,
        "--input_dir", input_dir,
        "--output_dir", output_dir,
        "--model_path", FIXED_PARAMS["yolo_roi_extractor"]["model_path"],
        "--roi_conf", str(FIXED_PARAMS["yolo_roi_extractor"]["roi_conf"]),
        "--roi_iou", str(FIXED_PARAMS["yolo_roi_extractor"]["roi_iou"]),
        "--padding", str(FIXED_PARAMS["yolo_roi_extractor"]["padding"]),
        "--mode", FIXED_PARAMS["yolo_roi_extractor"]["mode"]
    ]

    run_command(command, "YOLO ROIæå–")


def process_patch_enhance(input_dir: str, output_dir: str):
    """æ‰§è¡Œå›¾åƒè£å‰ªå¢å¼º"""
    script_path = get_abs_path(FIXED_PARAMS["patchandenhance"]["script_path"])
    command = [
        sys.executable, script_path,
        "--input_dir", input_dir,
        "--output_dir", output_dir,
        "--overlap", str(FIXED_PARAMS["patchandenhance"]["overlap"]),
        "--enhance_mode", FIXED_PARAMS["patchandenhance"]["enhance_mode"],
        "--window_size",
        str(FIXED_PARAMS["patchandenhance"]["window_size"][0]),
        str(FIXED_PARAMS["patchandenhance"]["window_size"][1]),
        "--label_mode", FIXED_PARAMS["patchandenhance"]["label_mode"]
    ]

    if FIXED_PARAMS["patchandenhance"]["no_slice"]:
        command.append("--no_slice")

    run_command(command, "å›¾åƒè£å‰ªä¸å¢å¼º")


def main():
    print("ğŸš€ æ•°æ®å¤„ç†æµæ°´çº¿å¯åŠ¨ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼")
    print(f"åŸºç¡€è·¯å¾„ï¼š{BASE_PATH}")
    print(f"å¾…å¤„ç†æ•°æ®é›†ï¼š{DATASETS}")

    # è·å– unify_to_crack è®¾ç½®
    unify_to_crack = FIXED_PARAMS["labelme2yolo"].get("unify_to_crack", False)
    if unify_to_crack:
        print("\nâš ï¸ æ³¨æ„ï¼šå·²å¯ç”¨ unify_to_crackï¼Œæ‰€æœ‰æ ‡ç­¾å°†è¢«ç»Ÿä¸€ä¸º 'crack'")

    # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰æ ‡ç­¾ï¼Œå»ºç«‹ç»Ÿä¸€æ˜ å°„
    label_map = collect_all_labels(DATASETS, JSON_BASE_PATH, unify_to_crack)

    if not label_map:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•æ ‡ç­¾ï¼")
        sys.exit(1)

    # ç¬¬äºŒæ­¥ï¼šç›´æ¥å¤„ç†æ‰€æœ‰æ•°æ®é›†åˆ°ä¸»ç›®å½•
    print("\n" + "=" * 100)
    print("ğŸ“ æ‰¹é‡å¤„ç† Labelme æ•°æ®ï¼ˆä½¿ç”¨ç»Ÿä¸€æ ‡ç­¾æ˜ å°„ï¼‰")
    print("=" * 100)

    process_all_datasets_directly(
        DATASETS,
        BASE_PATH,
        JSON_BASE_PATH,
        OUTPUT_CONFIG["yolo_dir"],
        label_map,
        FIXED_PARAMS["labelme2yolo"]
    )

    # ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡Œ ROI æå–
    print("\n" + "=" * 100)
    print("ğŸ“ æ‰§è¡Œ YOLO ROI åŒºåŸŸæå–")
    print("=" * 100)

    if not os.path.exists(OUTPUT_CONFIG["yolo_dir"]):
        print(f"âŒ é”™è¯¯ï¼šYOLO æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨ {OUTPUT_CONFIG['yolo_dir']}")
        sys.exit(1)

    process_roi_extractor(OUTPUT_CONFIG["yolo_dir"], OUTPUT_CONFIG["roi_dir"])

    # ç¬¬å››æ­¥ï¼šæ‰§è¡Œå›¾åƒè£å‰ªå¢å¼º
    print("\n" + "=" * 100)
    print("ğŸ“ æ‰§è¡Œå›¾åƒè£å‰ªä¸å¢å¼º")
    print("=" * 100)

    if not os.path.exists(OUTPUT_CONFIG["roi_dir"]):
        print(f"âŒ é”™è¯¯ï¼šROI æå–ç›®å½•ä¸å­˜åœ¨ {OUTPUT_CONFIG['roi_dir']}")
        sys.exit(1)

    process_patch_enhance(OUTPUT_CONFIG["roi_dir"], OUTPUT_CONFIG["patch_dir"])

    print("\n" + "ğŸ‰" * 50)
    print("ğŸ‰ æ‰€æœ‰æ•°æ®å¤„ç†æ­¥éª¤æ‰§è¡Œå®Œæˆï¼")
    print(f"ğŸ“ æœ€ç»ˆç»“æœä¿å­˜ç›®å½•ï¼š{OUTPUT_CONFIG['patch_dir']}")
    print("ğŸ‰" * 50)


if __name__ == "__main__":
    main()